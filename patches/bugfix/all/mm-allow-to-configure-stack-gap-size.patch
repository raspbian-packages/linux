From: Michal Hocko <mhocko@suse.com>
Date: Wed, 14 Jun 2017 08:17:02 +0200
Subject: mm: allow to configure stack gap size
Bug-Debian: https://security-tracker.debian.org/tracker/CVE-2017-1000364

Add a kernel command line option (stack_guard_gap) to specify the stack
gap size (in page unites) and export the value in /proc/<pid>/smaps for
stack vmas. This might be used for special applications like CRIU/RR.

Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
Signed-off-by: Michal Hocko <mhocko@suse.com>
[carnil: backport to 3.16
 - context adjustment
 - adjust location for documentation
 - is_stack -> vm_is_stack]
[bwh: Backported to 3.2: fold in later change to use has_gap, as we don't
 have vm_is_stack()
---
 Documentation/kernel-parameters.txt |  7 +++++++
 fs/proc/task_mmu.c                  | 21 ++++++++++++++++-----
 mm/mmap.c                           | 13 +++++++++++++
 3 files changed, 36 insertions(+), 5 deletions(-)

--- a/Documentation/kernel-parameters.txt
+++ b/Documentation/kernel-parameters.txt
@@ -2463,6 +2463,13 @@ bytes respectively. Such letter suffixes
 	spia_pedr=
 	spia_peddr=
 
+	stack_guard_gap=	[MM]
+			override the default stack gap protection. The value
+			is in page units and it defines how many pages prior
+			to (for stacks growing down) resp. after (for stacks
+			growing up) the main stack are reserved for no other
+			mapping. Default value is 256 pages.
+
 	stacktrace	[FTRACE]
 			Enabled the stack tracer on boot up.
 
--- a/fs/proc/task_mmu.c
+++ b/fs/proc/task_mmu.c
@@ -210,7 +210,8 @@ static int do_maps_open(struct inode *in
 	return ret;
 }
 
-static void show_map_vma(struct seq_file *m, struct vm_area_struct *vma)
+static void
+show_map_vma(struct seq_file *m, struct vm_area_struct *vma, bool *has_gap)
 {
 	struct mm_struct *mm = vma->vm_mm;
 	struct file *file = vma->vm_file;
@@ -232,11 +233,17 @@ static void show_map_vma(struct seq_file
 	start = vma->vm_start;
 	end = vma->vm_end;
 	if (vma->vm_flags & VM_GROWSDOWN) {
-		if (stack_guard_area(vma, start))
+		if (stack_guard_area(vma, start)) {
 			start += stack_guard_gap;
+			if (has_gap)
+				*has_gap = true;
+		}
 	} else if (vma->vm_flags & VM_GROWSUP) {
-		if (stack_guard_area(vma, end))
+		if (stack_guard_area(vma, end)) {
 			end -= stack_guard_gap;
+			if (has_gap)
+				*has_gap = true;
+		}
 	}
 
 	seq_printf(m, "%08lx-%08lx %c%c%c%c %08llx %02x:%02x %lu %n",
@@ -285,7 +292,7 @@ static int show_map(struct seq_file *m,
 	struct proc_maps_private *priv = m->private;
 	struct task_struct *task = priv->task;
 
-	show_map_vma(m, vma);
+	show_map_vma(m, vma, NULL);
 
 	if (m->count < m->size)  /* vma is copied successfully */
 		m->version = (vma != get_gate_vma(task->mm))
@@ -440,6 +447,7 @@ static int show_smap(struct seq_file *m,
 		.mm = vma->vm_mm,
 		.private = &mss,
 	};
+	bool has_gap = false;
 
 	memset(&mss, 0, sizeof mss);
 	mss.vma = vma;
@@ -447,7 +455,7 @@ static int show_smap(struct seq_file *m,
 	if (vma->vm_mm && !is_vm_hugetlb_page(vma))
 		walk_page_range(vma->vm_start, vma->vm_end, &smaps_walk);
 
-	show_map_vma(m, vma);
+	show_map_vma(m, vma, &has_gap);
 
 	seq_printf(m,
 		   "Size:           %8lu kB\n"
@@ -480,6 +488,9 @@ static int show_smap(struct seq_file *m,
 		   (vma->vm_flags & VM_LOCKED) ?
 			(unsigned long)(mss.pss >> (10 + PSS_SHIFT)) : 0);
 
+	if (has_gap)
+		seq_printf(m, "Stack_Gap:      %8lu kB\n", stack_guard_gap >>10);
+
 	if (m->count < m->size)  /* vma is copied successfully */
 		m->version = (vma != get_gate_vma(task->mm))
 			? vma->vm_start : 0;
--- a/mm/mmap.c
+++ b/mm/mmap.c
@@ -1799,6 +1799,19 @@ int expand_downwards(struct vm_area_stru
 /* enforced gap between the expanding stack and other mappings. */
 unsigned long stack_guard_gap = 256UL<<PAGE_SHIFT;
 
+static int __init cmdline_parse_stack_guard_gap(char *p)
+{
+	unsigned long val;
+	char *endptr;
+
+	val = simple_strtoul(p, &endptr, 10);
+	if (!*endptr)
+		stack_guard_gap = val << PAGE_SHIFT;
+
+	return 0;
+}
+__setup("stack_guard_gap=", cmdline_parse_stack_guard_gap);
+
 /*
  * Note how expand_stack() refuses to expand the stack all the way to
  * abut the next virtual mapping, *unless* that mapping itself is also
