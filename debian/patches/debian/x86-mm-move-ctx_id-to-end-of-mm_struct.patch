From: Ben Hutchings <ben@decadent.org.uk>
Date: Wed, 18 Apr 2018 00:56:56 +0100
Subject: x86/mm: Avoid ABI change for addition of ctx_id
Forwarded: not-needed

The new ctx_id field in mm_context_t will only be used by core
kernel code.  However, we need to move it to the end of the
structure to avoid changing offsets of the other fields.  And
mm_context_t gets embedded in struct mm_struct, so we need to
move it to the end of that struct instead.

---
--- a/arch/x86/include/asm/mmu.h
+++ b/arch/x86/include/asm/mmu.h
@@ -9,12 +9,6 @@
  * x86 has arch-specific MMU state beyond what lives in mm_struct.
  */
 typedef struct {
-	/*
-	 * ctx_id uniquely identifies this mm_struct.  A ctx_id will never
-	 * be reused, and zero is not a valid ctx_id.
-	 */
-	u64 ctx_id;
-
 #ifdef CONFIG_MODIFY_LDT_SYSCALL
 	struct ldt_struct *ldt;
 #endif
@@ -40,9 +34,7 @@ typedef struct {
 } mm_context_t;
 
 #define INIT_MM_CONTEXT(mm)						\
-	.context = {							\
-		.ctx_id = 1,						\
-	}
+	.x86_ctx_id = 1
 
 void leave_mm(int cpu);
 
--- a/arch/x86/include/asm/mmu_context.h
+++ b/arch/x86/include/asm/mmu_context.h
@@ -109,7 +109,7 @@ static inline void enter_lazy_tlb(struct
 static inline int init_new_context(struct task_struct *tsk,
 				   struct mm_struct *mm)
 {
-	mm->context.ctx_id = atomic64_inc_return(&last_mm_ctx_id);
+	mm->x86_ctx_id = atomic64_inc_return(&last_mm_ctx_id);
 
 	#ifdef CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS
 	if (cpu_feature_enabled(X86_FEATURE_OSPKE)) {
--- a/arch/x86/mm/tlb.c
+++ b/arch/x86/mm/tlb.c
@@ -125,7 +125,7 @@ void switch_mm_irqs_off(struct mm_struct
 		 * switch to a different non-dumpable process.
 		 */
 		if (tsk && tsk->mm &&
-		    tsk->mm->context.ctx_id != last_ctx_id &&
+		    tsk->mm->x86_ctx_id != last_ctx_id &&
 		    get_dumpable(tsk->mm) != SUID_DUMP_USER)
 			indirect_branch_prediction_barrier();
 
@@ -149,7 +149,7 @@ void switch_mm_irqs_off(struct mm_struct
 		 * to the same user.
 		 */
 		if (next != &init_mm)
-			this_cpu_write(cpu_tlbstate.last_ctx_id, next->context.ctx_id);
+			this_cpu_write(cpu_tlbstate.last_ctx_id, next->x86_ctx_id);
 
 		this_cpu_write(cpu_tlbstate.state, TLBSTATE_OK);
 		this_cpu_write(cpu_tlbstate.active_mm, next);
--- a/include/linux/mm_types.h
+++ b/include/linux/mm_types.h
@@ -523,6 +523,16 @@ struct mm_struct {
 	atomic_long_t hugetlb_usage;
 #endif
 	struct work_struct async_put_work;
+
+#ifndef __GENKSYMS__
+#ifdef CONFIG_X86
+	/*
+	 * ctx_id uniquely identifies this mm_struct.  A ctx_id will never
+	 * be reused, and zero is not a valid ctx_id.
+	 */
+	u64 x86_ctx_id;
+#endif
+#endif
 };
 
 static inline void mm_init_cpumask(struct mm_struct *mm)
